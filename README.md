
# ğŸ“½ï¸ Kudvisuals

**Kudvisuals** is a Python-based prototype designed to **convert audio content (like podcasts and stories) into visuals** using generative AI. This helps audio platforms increase **user screen time** by offering visual engagement alongside audio content.

---

## ğŸ“¸ Demo & Sample Outputs

- ğŸï¸ **[Project Demo Video](project demo video.mp4)** â€“ Full walkthrough of the app.
- ğŸ–¼ï¸ Sample generated images:
  - ![Generated Image 1](generatedimg_1.png)
  - ![Generated Image 2](generatedimg_2.png)
  - ![Generated Image 3](generatedimg_3.png)

- ğŸ” App screenshots:
  - ![App GUI](samplelook1.JPG)
  - ![Model Ready View](samplelook2.JPG)

---

## ğŸ’¡ Objective

Audio platforms often struggle to retain users due to passive engagement. This tool **visualizes** the spoken content, helping:
- Increase screen time ğŸ“±
- Keep listeners more focused ğŸ§
- Make audio content shareable and immersive ğŸ“¡

---

## âš™ï¸ Features

- ğŸ™ï¸ **Voice-to-Text** via microphone
- ğŸ“ Prompt box auto-filled by audio input
- ğŸ–¼ï¸ Text-to-Image generation using Stable Diffusion
- ğŸª„ Clean GUI using `CustomTkinter`
- ğŸ’¾ Saves all generated visuals as PNG files
- ğŸ”§ Uses `FFmpeg` (bundled) for audio backend functionality

---

## ğŸ§ª Microphone Setup

Before running the app, test your microphone:

```bash
python mictest.py
```

ğŸ” Check the printed list and update this in `app.py`:

```python
with sr.Microphone(device_index=2) as source:
```

Replace `2` with your actual mic index.

---

## ğŸš€ Getting Started

> ğŸ“Œ Requires **Python 3.10**  
> ğŸ’¾ Needs ~4GB free space (for model) and an active internet connection (only for first use)

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/Kudvisuals.git
cd Kudvisuals
```

### 2. Create and Activate Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate  # For Windows
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

> ğŸ’¡ If any installation fails, install packages one-by-one.

---

## ğŸ§¨ Torch Installation (Offline or Manual)

If `torch` fails to install via `pip`, use the provided `.whl` file:

```bash
pip install torch-1.12.1+cpu-cp310-cp310-win_amd64.whl
```

Make sure you're using **Python 3.10 64-bit**.

---

## ğŸ§  How It Works

1. Press **ğŸ™ï¸ Record Voice** to capture audio.
2. Converts audio to text using `SpeechRecognition` (FFmpeg backend).
3. Press **ğŸ–¼ï¸ Generate Image** to generate visuals using Stable Diffusion.
4. Image is displayed + saved automatically.

---

## ğŸ“‚ Model Download Path

On first run, if no local model is found, the Stable Diffusion model will be downloaded from Hugging Face.  
It will be stored in a default cache path like:

```
C:\Users\<your-username>\.cache\huggingface\hub\models--runwayml--stable-diffusion-v1-5
```

You can move or copy it to a folder named `stable-diffusion` in your project directory to avoid redownloading.

---

## ğŸ“ Project Structure

```
Kudvisuals/
â”‚
â”œâ”€â”€ app.py                      # Main app interface
â”œâ”€â”€ mictest.py                  # Mic testing script
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ torch-1.12.1+cpu-...whl     # Torch install file (optional)
â”œâ”€â”€ project demo video.mp4      # App demo video
â”œâ”€â”€ generatedimg_1.png          # Sample generated image
â”œâ”€â”€ generatedimg_2.png
â”œâ”€â”€ generatedimg_3.png
â”œâ”€â”€ samplelook1.JPG             # Screenshot - App GUI
â”œâ”€â”€ samplelook2.JPG             # Screenshot - Model Ready
â”œâ”€â”€ ffmpeg/                     # FFmpeg executables
â”‚   â”œâ”€â”€ ffmpeg.exe
â”‚   â”œâ”€â”€ ffplay.exe
â”‚   â””â”€â”€ ffprobe.exe
â””â”€â”€ stable-diffusion/           # (Optional) Local model folder
```

---

## ğŸ‘¨â€ğŸ’» Author

Created by **Shantanu Anand**  
ğŸ¯ GenAI Project â€“ Visualizing audio for better engagement and retention.
